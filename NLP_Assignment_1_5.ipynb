{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 5: Optimising pre-processing and feature extraction (30 marks)\n",
    "\n",
    "I have made some adjustments to the original jupyter notebook so that it can handle different Feature extraction and pre-processing techniques using the same script. I have created a list of configurations, which are iterated through and the current configuration is passed to each function. Each configuration is a dictionary, with entries which map to the Description of the configuration, and the flags for each pre-processing technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               \n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction function for (Presence of words) / (Bag of Words) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_vector(tokens, gfd, config):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    def presence():\n",
    "        feature_dict = {}\n",
    "        ut = set(tokens)\n",
    "        for token in tokens:\n",
    "            feature_dict[token] = 1\n",
    "        for token in ut:\n",
    "            gfd[token] = 1\n",
    "        return feature_dict\n",
    "    def bag_of_words():\n",
    "        feature_dict = {}\n",
    "        ut = set(tokens)\n",
    "        for token in tokens:\n",
    "            feature_dict[token] = feature_dict.get(token, 0) + 1\n",
    "        for token in ut:\n",
    "            gfd[token] = gfd.get(token, 0) + 1\n",
    "        return feature_dict\n",
    "        \n",
    "    if config.get(\"feature_extraction_method\") == \"presence\":\n",
    "        return presence()\n",
    "    elif config.get(\"feature_extraction_method\") == \"BoW\":\n",
    "            return bag_of_words()\n",
    "    else:\n",
    "        return bag_of_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Splitting\n",
    "\n",
    "I have kept the `load_data()`function the same as in Q1-4, however I now have two different implementations of the `split_and_preprocess_data()` function such that it can properly support the different feature extraction methods.\n",
    "\n",
    "**Split and preprocess data**:\n",
    "\n",
    "A. `split_and_preprocess_data(percentage, config, traind, traind1, testd, testd1, gfd)`: This function is designed to be used with configurations which use a `Presence of word` or `Bag of words` feature extraction model. I have added a few extra parameters, however this function works in a very similar way to the function in the other notebook. Here is what each of the parameters does:\n",
    "- *percentage*: Tells the function what ratio the `raw_data` should be divided into to create the `train_data` and `test_data` variables.\n",
    "- *config*: Gives the flags for each of the different preprocessing and feature extraction methods that are to be used.\n",
    "- *traind, traind1, testd, testd1*: These are empty lists which are filled in by this function. The parameters `traind` and `testd` just store the normal train-test splits with features and labels. `traind1` and `testd1` also store the original text sample alongside the features and the label which is convenient for certain operation which is explained further in later sections.\n",
    "- *gfd*: This is an empty dictionary which represents the global feature dictionary. \n",
    "\n",
    "\n",
    "B. `split_and_preprocess_data_tfidf(percentage, config)`: \n",
    "- This split and preprocess function is defined specifically to be used if the configuration uses a `TF/IDF` feature extraction method.\n",
    "- The parameters for this function works the same way as in the previous function.\n",
    "- For this feature extraction method we don't call the `to_feature_vector` function but directly make use of the `sklearn` function `TfidfVectorizer` to generate the features.\n",
    "- Here the data is split based on the percentage parameter and the tokens which are obtained from pre-processing the train and test data are joined together to create pre-processed text samples which are neccessary as `TfidfVectorizer` creates features from all documents at once.\n",
    "- I had to limit the `max_features` parameter otherwise I ended up facing memory issues and the jupyter kernel crashing.\n",
    "- The variables `tfidf_train` and `tfidf_test` contain the training and testing features created using the `TfidfVectorizer`. The `TfidfVectorizer` learns the vocabulary and then computes the associated TF/IDF values.\n",
    "- The function returns `traind`, `testd` and `tfidf_vectorizer`, where `traind` and `testd` are lists of tuples with features and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage, config, traind, traind1, testd, testd1, gfd):\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        traind.append((to_feature_vector(pre_process(text, config), gfd, config), label))\n",
    "        traind1.append((text, to_feature_vector(pre_process(text, config), gfd, config), label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        testd.append((to_feature_vector(pre_process(text, config), gfd, config), label))\n",
    "        testd1.append((text, to_feature_vector(pre_process(text, config), gfd, config), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def split_and_preprocess_data_tfidf(percentage, config):\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    \n",
    "    ## Creates a list of texts by joining together tokens obtained from preprocessing\n",
    "    training_texts = [\" \".join(pre_process(text, config)) for text, _ in raw_data[:num_training_samples]]\n",
    "    test_texts = [\" \".join(pre_process(text, config)) for text, _ in raw_data[num_training_samples:]]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50_000)\n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(training_texts)\n",
    "    tfidf_test = tfidf_vectorizer.transform(test_texts)\n",
    "\n",
    "    # Puts the Training and Testing features created from samples with their associated labels \n",
    "    traind = [(tfidf_train[i], label) for i, (_, label) in enumerate(raw_data[:num_training_samples])]\n",
    "    testd = [(tfidf_test[i], label) for i, (_, label) in enumerate(raw_data[num_training_samples:])]\n",
    "\n",
    "    return traind, testd, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just positive or negative and the statement\n",
    "    # e.g. (label, statement)\n",
    "    _, label, statement = data_line\n",
    "    return (label, statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for Feature Extraction and Pre-processing and populating `raw_data` \n",
    "\n",
    "I have listed all 26 configurations which I tested but have commented the ones which I didn't include in the report. You can enable or disable configurations by commenting or uncommenting lines from the configuration list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawData 33540\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "# 26 Configurations with different combination of preprocessing and feature extraction techniques\n",
    "configurations = [\n",
    "    {\"description\": \"Presence/Absence of Word + Punctuation Separation\"                                                                           , \"feature_extraction_method\": \"presence\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Punctuation Separation\"                                                                                       , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + URL Removal + Punctuation Separation\"                                                                         , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Punctuation Removal\"                                                                                          , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Punctuation Separation + Stemming\"                                                                            , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":True , \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Punctuation Separation + Lemmatization\"                                                                       , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + URL Removal + Punctuation Separation + Stemming\"                                                              , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":True , \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + URL Removal + Punctuation Separation + Lemmatization\"                                                         , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Bigrams + Punctuation Removal\"                                                                                , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Bigrams + Trigrams + URL Removal + Punctuation Separation\"                                                    , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":True , \"add_tg\":True , \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Bigrams + Trigrams + URL Removal + Punctuation Separation + Lemmatization\"                                    , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":True , \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Trigrams + Punctuation Removal + Lemmatization\"                                                               , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":False, \"add_tg\":True , \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Trigrams + Punctuation Separation + Lemmatization\"                                                            , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":True , \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Bigrams + Punctuation Separation + Lemmatization\"                                                             , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Punctuation Separation + Stopword Removal\"                                                                    , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Punctuation Separation + Lemmatization + Stopword Removal\"                                                    , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":True , \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Bigrams + Punctuation Separation + Lemmatization + Stopword Removal\"                                          , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Quadgrams + Punctuation Separation\"                                                                           , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":True },\n",
    "    # {\"description\": \"Bag of words + Bigrams + Trigrams + Quadgrams + Punctuation Separation\"                                                      , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":True , \"rmt\": False, \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":True , \"add_tg\":True , \"add_qg\":True },\n",
    "    {\"description\": \"Bag of words + Bigrams + Punctuation Removal + Tag Removal + Lemmatization\"                                                  , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": True , \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    {\"description\": \"Bag of words + Bigrams + Trigrams + Quadgrams + URL Removal + Punctuation Removal + Lemmatization + Stopword Removal\"        , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":True , \"sep_pn\":False, \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":True , \"add_qg\":True },\n",
    "    {\"description\": \"Bag of words + Bigrams + Punctuation Removal + Lemmatization + Stopword Removal\"                                             , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"Bag of words + Trigrams + Punctuation Removal + Lemmatization + Stopword Removal\"                                            , \"feature_extraction_method\": \"BoW\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": False, \"rm_sw\":True , \"app_stem\":False, \"app_lem\":True , \"add_bg\":False, \"add_tg\":True , \"add_qg\":False},\n",
    "    # {\"description\": \"TFIDF + Punctuation Separation\"                                                                                              , \"feature_extraction_method\": \"tfidf\" , \"rm_urls\":False, \"sep_pn\":True, \"rmt\": False , \"rm_sw\":False, \"app_stem\":False, \"app_lem\":False, \"add_bg\":False, \"add_tg\":False, \"add_qg\":False},\n",
    "    {\"description\": \"TFIDF + Bigrams + Punctuation Removal + Tag Removal + Lemmatization\"                                                         , \"feature_extraction_method\": \"tfidf\" , \"rm_urls\":False, \"sep_pn\":False, \"rmt\": True , \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "    # {\"description\": \"TFIDF + Bigrams + Trigrams + Quadgrams + URL Removal + Punctuation Removal + Tag Removal + Lemmatization + Stopword Removal\" , \"feature_extraction_method\": \"tfidf\" , \"rm_urls\":True,  \"sep_pn\":False, \"rmt\": True , \"rm_sw\":False, \"app_stem\":False, \"app_lem\":True , \"add_bg\":True , \"add_tg\":False, \"add_qg\":False},\n",
    "]\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "print(\"rawData\", len(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Text Samples\n",
    "\n",
    "These are the pre-processing techniques which I have employed and I have also included some feature extraction methods within the same function (n-grams implementation), as it made it more convenient to return the tokens for the text all in one go. These are the techniques which I used:\n",
    "- URL Removal\n",
    "- Punctuation Separation\n",
    "- Punctuation Removal\n",
    "- User tag and Hash tag removal\n",
    "- Applying Porter-Stemmer\n",
    "- Applying Lemmatization\n",
    "- Stop word removal\n",
    "- Generating N-grams (Bigrams, Trigrams and Quadgrams)\n",
    "\n",
    "I check the specific configuration flag for each of the above techniques and apply them accordingly. Once all of the different preprocessing techniques have been applied, the pre-processed text is tokenized. This is followed by the generation of n-grams (if they are part of the configuration passed to the function) which are appended to the list of tokens (or unigrams). After this the final list of tokens is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text, config):\n",
    "    \"\"\" \n",
    "    Performs Different preprocessing operations based on the config parameter passed to the function.\n",
    "\n",
    "    Parameters:\n",
    "    text (string): passes a line of text (assume sentence segmentation has already been done)\n",
    "    config (dictionary): passes a bunch of flags indicating which techniques are to be used\n",
    "\n",
    "    Returns:\n",
    "    List[string]: Should return a list of tokens.\n",
    "    \"\"\"\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "\n",
    "    def remove_urls(text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        cleaned_text = re.sub(url_pattern, '', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text) # separates punctuation at ends of strings and separates hash tags\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text) # separates punctuation at beginning of strings and separates hash tags\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1\", text) # removes punctuation at ends of strings and removes hash tags\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\2\", text) # removes punctuation at beginning of strings and removes hash tags\n",
    "        return text\n",
    "    \n",
    "    def remove_tags(text):\n",
    "        text = re.sub(r\"(\\w)([@#'\\\"”\\)])\", r\"\\1\", text) # removes user tags and hash tags at ends of strings\n",
    "        text = re.sub(r\"([@#'\\\"“\\(\\)])(\\w)\", r\"\\2\", text) # removes user tags  and hash tags at beginning of strings\n",
    "        return text\n",
    "        \n",
    "    def tokenize_text(text):\n",
    "        tokens = re.split(r\"\\s+\",text) # separate into tokens by splitting on trailing spaces\n",
    "        # normalization - only by lower casing for now\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        # Use porter stemmer\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        # Apply lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        # make n-grams from tokens\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    # Remove URLs\n",
    "\n",
    "    if config.get(\"rm_urls\"):\n",
    "        text = remove_urls(text)\n",
    "\n",
    "    # Separate Punctuation otherwise Remove it\n",
    "    \n",
    "    if config.get(\"sep_pn\"):\n",
    "        text = separate_punctuation(text)\n",
    "    else:\n",
    "        text = remove_punctuation(text)\n",
    "    \n",
    "    # Remove User and Hash tags\n",
    "\n",
    "    if config.get(\"rmt\"):\n",
    "        text = remove_tags(text)\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Apply Lemmatization or Stemming\n",
    "\n",
    "    if config.get(\"app_stem\"):\n",
    "        tokens = apply_stemming(tokens)\n",
    "\n",
    "    if config.get(\"app_lem\"):\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "\n",
    "\n",
    "    # Generate bigrams, trigrams and quadgrams\n",
    "    if config.get(\"add_bg\"):\n",
    "        bigrams = generate_ngrams_from_tokens(tokens, 2)\n",
    "        bg = [i + \" \" + j for (i,j) in bigrams]\n",
    "        tokens += bg\n",
    "\n",
    "    if config.get(\"add_tg\"):\n",
    "        trigrams = generate_ngrams_from_tokens(tokens, 3)\n",
    "        tg = [i + \" \" + j + \" \" + k for (i,j,k) in trigrams]\n",
    "        tokens += tg\n",
    "\n",
    "    if config.get(\"add_qg\"):\n",
    "        quadgrams = generate_ngrams_from_tokens(tokens, 4)\n",
    "        qg = [i + \" \" + j + \" \" + k + \" \" + l for (i,j,k,l) in quadgrams]\n",
    "        tokens += qg\n",
    "\n",
    "    # Remove Stop words\n",
    "    if config.get(\"rm_sw\"):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifier based on Feature Extraction Method\n",
    "\n",
    "- The method for training the classifier for `Bag of Words` and `Presence of Words` feature extraction model is the same as the code provided in the template for questions 1-4.\n",
    "- However for the \"TF/IDF\" feature extraction method, I had to adapt the training function such that it works for the features generated by the `TfidfVectorizer` in the `split_and_preprocess_data_tfidf()` function.\n",
    "    - I imported vstack from scipy.sparse so that a sparse matrix can be created for training the classifier. The features are taken from the data into a matrix where each row represents the documents and each column represents the terms. The sparse matrix has been used for efficient memory usage as matrices from TF/IDF features can be quite large.\n",
    "    - The LinearSVC model is then trained on the sparse matrix and the extracted labels.\n",
    "\n",
    "I had experimented with the `C` parameter and tried using `class_weight=balanced` for LinearSVC, however it didn't result in any gains and sometimes even made the model worse so I have chosen not to include it in the final implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "def train_classifier(data, config):\n",
    "    if config.get(\"feature_extraction_method\") == \"BoW\" or config.get(\"feature_extraction_method\") == \"presence\":\n",
    "        print(\"(BoW/presence)Training Classifier...\")\n",
    "        pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "        return SklearnClassifier(pipeline).train(data)\n",
    "    else:\n",
    "        print(\"(TF/IDF)Training Classifier...\")\n",
    "        # Stack sparse matrices to create a 2D sparse matrix for LinearSVC\n",
    "        X_train = vstack([sample[0] for sample in data])  # vstack is used to keep X_train in sparse format\n",
    "        y_train = [sample[1] for sample in data]  # Extract labels\n",
    "        \n",
    "        model = LinearSVC()\n",
    "        model.fit(X_train, y_train)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dataset, folds, config):\n",
    "    results = []\n",
    "    fold_size = int(len(dataset)/folds) + 1\n",
    "    best_fold = None\n",
    "    best_accuracy = 0\n",
    "    for i in range(0,len(dataset),int(fold_size)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        # print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        train_data, test_data  = dataset[:i]+dataset[i+fold_size:], dataset[i:i+fold_size]\n",
    "        test_inputs = [data[0] for data in test_data]\n",
    "        test_labels = [data[1] for data in test_data]\n",
    "        classifier = train_classifier(train_data, config)\n",
    "        predicted_labels = predict_labels(test_inputs, classifier, config)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(test_labels, predicted_labels, average=\"weighted\")\n",
    "        accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "        cv_results = [precision, recall, fscore, accuracy]\n",
    "        results.append(cv_results)\n",
    "        print(accuracy)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_fold = cv_results\n",
    "    return results, best_fold, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Labels for Different Feature Extraction Methods\n",
    "- `classifier.classify_many(samples)` is used for the `Presence of Words` and `Bag of Words` implementation just like the implementation for questions 1-4.\n",
    "- However for `TF/IDF` method, `classifier.predict(samples)` is used instead. This is mainly because the classifier is trained on a sparse matrix. `.predict()` function supports sparse matrix inputs and thus was a natural choice for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier, config):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    if config.get(\"feature_extraction_method\") == \"presence\" or config.get(\"feature_extraction_method\") == \"BoW\":\n",
    "        return classifier.classify_many(samples)\n",
    "    elif config.get(\"feature_extraction_method\") == \"tfidf\":\n",
    "        return classifier.predict(samples)\n",
    "\n",
    "\n",
    "# def predict_label_from_raw(sample, classifier):\n",
    "#     \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "#     return classifier.classify(to_feature_vector(pre_process(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_heatmap(y_test, preds, labels):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    cm = metrics.confusion_matrix(y_test, preds, labels=labels)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=45)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Script to run all scenarios and get all results at once!\n",
    "\n",
    "Just to quickly remind what `traind`, `traind1`, `testd`, `testd1`, `gfd` are:\n",
    "- `traind` and `testd`: Contains a list of tuples with the first element being a feature and the second being the associated label. `traind` contains all of the feature-label tuples used for training while `testd` contains all of the feature-label tuples used for testing. The train and test data was split in one of the `split_and_preprocess_data` functions.\n",
    "- `traind1` and `testd1`: Contains tuples with the original text samples alongside the features and labels. These variables have the same features and labels which are also part of `traind` and `testd`.\n",
    "- `gfd`: This is the Global Feature Dictionary.\n",
    "\n",
    "\n",
    "1. Here the metrics can be computed for both the first fold as well as through the full cross-validation. The preferred method would be to use the first fold of the cross-validation to speed up the computation.\n",
    "<!-- 1. Here I'm performing just the first fold of cross validation as it's the method I used for error analysis and it's also quicker than running the cross-validation function which would run for all 26 cases in one go which would take forever to execute. -->\n",
    "2. If anyone wants to check the check the cross_validation results for each case, the code for it has been just commented right above so one can uncomment it and test that version as well.\n",
    "    - In that case comment the lines `fold_size = ...` to `accuracy = accuracy_score(test_labels, predicted_labels)`, before you run the code for cross-validation.\n",
    "3. I return the text description of each configuration alongside all of the performance metrics inside a dictionary. This way I can convert the results into a panda DataFrame later which makes it nicer to see all of the data for each of the cases in one place.\n",
    "4. I also put the code for training on the entire training data right below the normal computation if one wants to see how the model performs in that case. Set `functions_complete` variable to True for the if and elif statements to see how it works for each of the 26 different cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_configurations(configurations):\n",
    "    results = []\n",
    "    for i, config in enumerate(configurations):\n",
    "        print(f\"Experiment {i} \", end=\"\")\n",
    "        traind, traind1, testd, testd1, gfd = [], [], [], [], {}\n",
    "\n",
    "        if config.get(\"feature_extraction_method\") == \"BoW\" or config.get(\"feature_extraction_method\") == \"presence\":\n",
    "            split_and_preprocess_data(0.8, config, traind, traind1, testd, testd1, gfd)\n",
    "\n",
    "            ## If anyone wants to try how the different configurations perform if all folds of cross validation are run. \n",
    "            ## WARNING -> This can run for upto 20+ minutes if run with all configurations enabled. \n",
    "            ## However you can reduce the amount of time by commenting out some of the cases from the configuration list.\n",
    "            \n",
    "            # cv = cross_validate(traind, 10, config)\n",
    "            # precision, recall, fscore, accuracy = cv[1]\n",
    "\n",
    "            fold_size = int(len(traind)/10) + 1\n",
    "            train_data_2, test_data_2  = traind1[fold_size:], traind1[:fold_size]\n",
    "            test_text, test_inputs, test_labels = [data[0] for data in test_data_2], [data[1] for data in test_data_2], [data[2] for data in test_data_2]\n",
    "            classifier = train_classifier(traind[fold_size:], config)\n",
    "            predicted_labels = predict_labels(test_inputs, classifier, config)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(test_labels, predicted_labels, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "            results.append({\n",
    "                \"config\": config[\"description\"],\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": fscore,\n",
    "                \"accuracy\": accuracy,\n",
    "            })\n",
    "\n",
    "            \n",
    "            # Finally, check the accuracy of your classifier by training on all the traning data\n",
    "            # and testing on the test set\n",
    "            functions_complete = False  # set to True once you're happy with your methods for cross val\n",
    "            if functions_complete:\n",
    "                print(testd[0])   # have a look at the first test data instance\n",
    "                classifier = train_classifier(traind, config)  # train the classifier\n",
    "                test_true = [t[1] for t in testd]   # get the ground-truth labels from the data\n",
    "                test_pred = predict_labels([x[0] for x in testd], classifier, config)  # classify the test data to get predicted labels\n",
    "                precision, recall, fscore, _ = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "                accuracy = accuracy_score(test_true, test_pred)\n",
    "                print(\"Done training!\")\n",
    "                print(f\"Precision: {precision} | Recall: {recall} | F Score:{fscore} | Accuracy:{accuracy}\")\n",
    "        \n",
    "        elif config.get(\"feature_extraction_method\") == \"tfidf\":\n",
    "            traind, testd, tfidf_vectorizer = split_and_preprocess_data_tfidf(0.8, config)\n",
    "\n",
    "            fold_size = int(len(traind)/10) + 1\n",
    "            train_data_2, test_data_2  = traind[fold_size:], traind[:fold_size]\n",
    "            test_inputs, test_labels = vstack([data[0] for data in test_data_2]), [data[1] for data in test_data_2]\n",
    "            classifier = train_classifier(traind[fold_size:], config)\n",
    "            predicted_labels = predict_labels(test_inputs, classifier, config)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(test_labels, predicted_labels, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "            results.append({\n",
    "                \"config\": config[\"description\"],\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": fscore,\n",
    "                \"accuracy\": accuracy,\n",
    "            })\n",
    "\n",
    "            # Finally, check the accuracy of your classifier by training on all the traning data\n",
    "            # and testing on the test set\n",
    "            functions_complete = False  # set to True once you're happy with your methods for cross val\n",
    "            if functions_complete:\n",
    "                print(testd[0])   # have a look at the first test data instance\n",
    "                test_inputs = vstack([data[0] for data in testd])\n",
    "                test_true = [data[1] for data in testd]\n",
    "                classifier = train_classifier(traind, config)\n",
    "                test_pred = predict_labels(test_inputs, classifier, config)\n",
    "                precision, recall, fscore, _ = precision_recall_fscore_support(test_true, test_pred, average='weighted')\n",
    "                accuracy = accuracy_score(test_true, test_pred)\n",
    "                print(\"Done training!\")\n",
    "                print(f\"Precision: {precision} | Recall: {recall} | F Score:{fscore} | Accuracy:{accuracy}\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 1 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 2 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 3 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 4 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 5 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 6 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 7 (BoW/presence)Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 8 (BoW/presence)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Experiment 9 (TF/IDF)Training Classifier...\n",
      "--------------------------------------------------------------------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run all configurations for this dataset\n",
    "# WARNING: This function will take upto 5 minutes or more to execute if run with all configurations (with first fold of cross-validation)\n",
    "results = run_all_configurations(configurations)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Results!\n",
    "\n",
    "These results are discussed in the report, explaining which approaches performed better and which ones underperformed and potential reasons for why that might have happened. I have stored the results in the file \"Final_Results.xlsx\" for all of the cases I tested for so please check that to see the metrics for all results in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presence/Absence of Word + Punctuation Separation</td>\n",
       "      <td>0.853747</td>\n",
       "      <td>0.856185</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>0.856185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag of words + Punctuation Separation</td>\n",
       "      <td>0.855645</td>\n",
       "      <td>0.858048</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>0.858048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag of words + Bigrams + Punctuation Removal</td>\n",
       "      <td>0.870122</td>\n",
       "      <td>0.872206</td>\n",
       "      <td>0.869572</td>\n",
       "      <td>0.872206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bag of words + Trigrams + Punctuation Removal ...</td>\n",
       "      <td>0.868971</td>\n",
       "      <td>0.871088</td>\n",
       "      <td>0.868345</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag of words + Bigrams + Punctuation Separatio...</td>\n",
       "      <td>0.866994</td>\n",
       "      <td>0.869225</td>\n",
       "      <td>0.866588</td>\n",
       "      <td>0.869225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bag of words + Quadgrams + Punctuation Separation</td>\n",
       "      <td>0.868549</td>\n",
       "      <td>0.870343</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>0.870343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag of words + Bigrams + Punctuation Removal +...</td>\n",
       "      <td>0.877065</td>\n",
       "      <td>0.878912</td>\n",
       "      <td>0.876735</td>\n",
       "      <td>0.878912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bag of words + Bigrams + Trigrams + Quadgrams ...</td>\n",
       "      <td>0.843002</td>\n",
       "      <td>0.841282</td>\n",
       "      <td>0.831153</td>\n",
       "      <td>0.841282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bag of words + Bigrams + Punctuation Removal +...</td>\n",
       "      <td>0.867455</td>\n",
       "      <td>0.869598</td>\n",
       "      <td>0.866647</td>\n",
       "      <td>0.869598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF + Bigrams + Punctuation Removal + Tag Re...</td>\n",
       "      <td>0.863522</td>\n",
       "      <td>0.865872</td>\n",
       "      <td>0.863604</td>\n",
       "      <td>0.865872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              config  precision    recall  \\\n",
       "0  Presence/Absence of Word + Punctuation Separation   0.853747  0.856185   \n",
       "1              Bag of words + Punctuation Separation   0.855645  0.858048   \n",
       "2       Bag of words + Bigrams + Punctuation Removal   0.870122  0.872206   \n",
       "3  Bag of words + Trigrams + Punctuation Removal ...   0.868971  0.871088   \n",
       "4  Bag of words + Bigrams + Punctuation Separatio...   0.866994  0.869225   \n",
       "5  Bag of words + Quadgrams + Punctuation Separation   0.868549  0.870343   \n",
       "6  Bag of words + Bigrams + Punctuation Removal +...   0.877065  0.878912   \n",
       "7  Bag of words + Bigrams + Trigrams + Quadgrams ...   0.843002  0.841282   \n",
       "8  Bag of words + Bigrams + Punctuation Removal +...   0.867455  0.869598   \n",
       "9  TFIDF + Bigrams + Punctuation Removal + Tag Re...   0.863522  0.865872   \n",
       "\n",
       "   f1_score  accuracy  \n",
       "0  0.854294  0.856185  \n",
       "1  0.856153  0.858048  \n",
       "2  0.869572  0.872206  \n",
       "3  0.868345  0.871088  \n",
       "4  0.866588  0.869225  \n",
       "5  0.866806  0.870343  \n",
       "6  0.876735  0.878912  \n",
       "7  0.831153  0.841282  \n",
       "8  0.866647  0.869598  \n",
       "9  0.863604  0.865872  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
